# -*- coding: utf-8 -*-
"""Project Notebook Sem 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DgHt9AfQF8QOpgnw_d94W9e_mYY8E8h9
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras import backend
from tensorflow.keras import layers
from tensorflow.keras import Model

import cv2
import numpy as np
import os
import math

import matplotlib.pyplot as plt
from tensorflow.keras.optimizers import *
from tensorflow.keras.callbacks import *
from tensorflow.keras.applications import *
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import joblib
from PIL import Image
from PIL import ImageFilter

import random
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing.image import load_img
from keras.preprocessing import image

import concurrent.futures
import warnings
warnings.filterwarnings('ignore')

IMG_SIZE=224

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error as mae
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def whiteblack(img):
    wb = Image.fromarray(img.astype(np.uint8)).filter(ImageFilter.EDGE_ENHANCE)
    wb = wb.filter(ImageFilter.EDGE_ENHANCE)
    wb= np.asarray(wb, dtype='float32')
    wb = cv2.cvtColor(wb, cv2.COLOR_BGR2GRAY)
    wb = np.asarray(wb, dtype='float32')*(-1) + 255
    wb = cv2.fastNlMeansDenoising(wb.astype('uint8'), h=20, templateWindowSize=7, searchWindowSize=21)
    wb= cv2.bilateralFilter(wb.astype('uint8'), 3, 16, 4)
    wb = cv2.medianBlur(wb,3)
    return wb

def adaptivegauss(img):
    bw_1 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    th3 = bw_1.astype('uint8')
    adapt_gauss = cv2.adaptiveThreshold(th3,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\
            cv2.THRESH_BINARY,11,2)
    adapt_gauss = cv2.medianBlur(adapt_gauss,3)

    # print(adapt_gauss.shape)
    fig = plt.figure()
    plt.imshow(adapt_gauss)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/adapt_gauss.jpg', dpi = 500)

    return adapt_gauss

def seg32(img):
    bw_1 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    th3 = bw_1.astype('uint8')
    bw_32seg = cv2.fastNlMeansDenoising(th3, None, h=5, templateWindowSize=7, searchWindowSize=21)
    for i in range(bw_32seg.shape[0]):
        for j in range(bw_32seg.shape[1]):
            if(bw_32seg[i, j]<32):
                bw_32seg[i, j]=0
            elif (bw_32seg[i, j]<64):
                bw_32seg[i, j]=32
            elif (bw_32seg[i, j]<32*3):
                bw_32seg[i, j]= 64
            elif (bw_32seg[i, j]<32*4):
                bw_32seg[i, j]=32*3
            elif (bw_32seg[i, j]<32*5):
                bw_32seg[i, j]=32*4
            elif (bw_32seg[i, j]<32*6):
                bw_32seg[i, j]=32*5
            elif (bw_32seg[i, j]<32*7):
                bw_32seg[i, j]=32*6
            elif (bw_32seg[i, j]<32*8):
                bw_32seg[i, j]=32*37
      
    # print(bw_32seg.shape)
    #fig = plt.figure()
    #plt.imshow(bw_32seg)
    #plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/bw_32seg.jpg', dpi = 500)
    
    return bw_32seg

def preprocess(img):
    img=cv2.resize(img,(IMG_SIZE,IMG_SIZE))
    img= np.asarray(img, dtype='float32')
    wb = whiteblack(img)
    image= np.tile(wb, (3, 1)).reshape(IMG_SIZE, IMG_SIZE, 3)
    adapt_gauss = adaptivegauss(img)
    seg_32= seg32(img)
    image[:, :, 0]= wb
    image[:, :, 1]= adapt_gauss
    image[:, :, 2]= seg_32
    image = (image/ 127.5) - 1

    # print(image.shape)
    fig = plt.figure()
    plt.imshow(image)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/preprocessed_image.jpg', dpi = 500)

    return image

def fill(img, h, w):
    img = cv2.resize(img, (h, w), cv2.INTER_CUBIC)
    return img
        
def horizontal_shift(img, ratio=0.2):
    ratio = random.uniform(-ratio, ratio)
    h, w = img.shape[:2]
    to_shift = w*ratio
    if ratio > 0:
        img = img[:, :int(w-to_shift), :]
    if ratio < 0:
        img = img[:, int(-1*to_shift):, :]
    img = fill(img, h, w)
    return img

def vertical_shift(img, ratio=0.2):
    if ratio > 1 or ratio < 0:
        print('Value should be less than 1 and greater than 0')
        return img
    ratio = random.uniform(-ratio, ratio)
    h, w = img.shape[:2]
    to_shift = h*ratio
    if ratio > 0:
        img = img[:int(h-to_shift), :, :]
    if ratio < 0:
        img = img[int(-1*to_shift):, :, :]
    img = fill(img, h, w)
    return img

def brightness(img, low=0.7, high=0.9):
    value = random.uniform(low, high)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    hsv = np.array(hsv, dtype = np.float64)
    hsv[:,:,1] = hsv[:,:,1]*value
    hsv[:,:,1][hsv[:,:,1]>255]  = 255
    hsv[:,:,2] = hsv[:,:,2]*value 
    hsv[:,:,2][hsv[:,:,2]>255]  = 255
    hsv = np.array(hsv, dtype = np.uint8)
    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
    return img

def channel_shift(img, value=40):
    value = int(random.uniform(-value, value))
    img = img + value
    img[:,:,:][img[:,:,:]>255]  = 255
    img[:,:,:][img[:,:,:]<0]  = 0
    img = img.astype(np.uint8)
    return img

def erosion_image(image,shift=2):
    kernel = np.ones((shift,shift),np.uint8)
    image = cv2.erode(image,kernel,iterations = 1)
    return image


def black_hat_image(image, shift=200):
    kernel = np.ones((shift, shift), np.uint8)
    image = cv2.morphologyEx(image, cv2.MORPH_BLACKHAT, kernel)
    return image

def gausian_blur(image,blur=1.5):
    image = cv2.GaussianBlur(image,(5,5),blur)
    return image

def averageing_blur(image,shift=5):
    image=cv2.blur(image,(shift,shift))
    return image

def sharpen_image(image):
    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
    image = cv2.filter2D(image, -1, kernel)
    return image

'''def gaussian_noise(image, var=50):
    row,col,ch= image.shape
    image= np.asarray(image, dtype='float32')
    mean = 10
    sigma = var**0.5
    gauss = np.random.normal(mean,sigma,(row,col,ch))
    gauss = gauss.reshape(row,col,ch)
    noisy = image + gauss
    noisy = np.asarray(noisy, dtype='uint8')
    noisy = np.clip(noisy, 0, 255)
    return noisy
  
def salt_image(image,p=0.05,a=0.06):
    noisy=image
    num_salt = np.ceil(a * image.size * p)
    coords = [np.random.randint(0, i - 1, int(num_salt))
              for i in image.shape]
    noisy[coords] = 1
    return noisy
  
def contrast_image(image,contrast=30):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    image[:,:,2] = [[max(pixel - contrast, 0) if pixel < 190 else min(pixel + contrast, 255) for pixel in row] for row in image[:,:,2]]
    image= cv2.cvtColor(image, cv2.COLOR_HSV2BGR)
    return image

'''

def fill(img, h, w):
    img = cv2.resize(img, (h, w), cv2.INTER_CUBIC)
    fig = plt.figure()
    plt.title('Fill')
    plt.imshow(img)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/fill.jpg', dpi = 500)

    return img
        
def horizontal_shift(img, ratio=0.2):
    ratio = random.uniform(-ratio, ratio)
    h, w = img.shape[:2]
    to_shift = w*ratio
    if ratio > 0:
        img = img[:, :int(w-to_shift), :]
    if ratio < 0:
        img = img[:, int(-1*to_shift):, :]
    img = fill(img, h, w)
    # print(img.shape)
    fig = plt.figure()
    plt.title('Horizontal shift')
    plt.imshow(img)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/horizontal_shift.jpg', dpi = 500)

    return img

def vertical_shift(img, ratio=0.2):
    if ratio > 1 or ratio < 0:
        print('Value should be less than 1 and greater than 0')
        return img
    ratio = random.uniform(-ratio, ratio)
    h, w = img.shape[:2]
    to_shift = h*ratio
    if ratio > 0:
        img = img[:int(h-to_shift), :, :]
    if ratio < 0:
        img = img[int(-1*to_shift):, :, :]
    img = fill(img, h, w)
    
    # print(img.shape)
    fig = plt.figure()
    plt.title('Vertical shift')
    plt.imshow(img)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/vertical_shift.jpg', dpi = 500)

    return img

def brightness(img, low=0.7, high=0.9):
    value = random.uniform(low, high)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    hsv = np.array(hsv, dtype = np.float64)
    hsv[:,:,1] = hsv[:,:,1]*value
    hsv[:,:,1][hsv[:,:,1]>255]  = 255
    hsv[:,:,2] = hsv[:,:,2]*value 
    hsv[:,:,2][hsv[:,:,2]>255]  = 255
    hsv = np.array(hsv, dtype = np.uint8)
    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
    
    # print(img.shape)
    fig = plt.figure()
    plt.title('Brightness')
    plt.imshow(img)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/brightness.jpg', dpi = 500)

    return img

def channel_shift(img, value=40):
    value = int(random.uniform(-value, value))
    img = img + value
    img[:,:,:][img[:,:,:]>255]  = 255
    img[:,:,:][img[:,:,:]<0]  = 0
    img = img.astype(np.uint8)
    # print(img.shape)
    fig = plt.figure()
    plt.title('Channel Shift')
    plt.imshow(img)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/channel_shift.jpg', dpi = 500)

    return img

def erosion_image(image,shift=2):
    kernel = np.ones((shift,shift),np.uint8)
    image = cv2.erode(image,kernel,iterations = 1)
    
    # print(img.shape)
    fig = plt.figure()
    plt.title('Erosion Image')
    plt.imshow(image)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/erosion_image.jpg', dpi = 500)

    return image


def black_hat_image(image, shift=200):
    kernel = np.ones((shift, shift), np.uint8)
    image = cv2.morphologyEx(image, cv2.MORPH_BLACKHAT, kernel)
    # print(img.shape)
    fig = plt.figure()
    plt.title('Black hat image')
    plt.imshow(image)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/black_hat_shift.jpg', dpi = 500)

    return image

def gausian_blur(image,blur=1.5):
    image = cv2.GaussianBlur(image,(5,5),blur)
    # print(img.shape)
    fig = plt.figure()
    plt.title('Gaussian Blur')
    plt.imshow(image)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/gaussian_blur.jpg', dpi = 500)
    

    return image

def averageing_blur(image,shift=5):
    image=cv2.blur(image,(shift,shift))
    # print(img.shape)
    fig = plt.figure()
    plt.title('Averaging blur')
    plt.imshow(image)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/averaging_shift.jpg', dpi = 500)

    return image

def sharpen_image(image):
    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
    image = cv2.filter2D(image, -1, kernel)
    # print(img.shape)
    fig = plt.figure()
    plt.title('Sharpened image')
    plt.imshow(image)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/sharpen_image.jpg', dpi = 500)

    return image

def gaussian_noise(image, var=50):
    row,col,ch= image.shape
    image= np.asarray(image, dtype='float32')
    mean = 10
    sigma = var**0.5
    gauss = np.random.normal(mean,sigma,(row,col,ch))
    gauss = gauss.reshape(row,col,ch)
    noisy = image + gauss
    noisy = np.asarray(noisy, dtype='uint8')
    noisy = np.clip(noisy, 0, 255)
    # print(img.shape)
    fig = plt.figure()
    plt.title('Gaussian Noise')
    plt.imshow(noisy)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/gaussian_noise.jpg', dpi = 500)

    return noisy
  
def salt_image(image,p=0.05,a=0.06):
    noisy=image
    num_salt = np.ceil(a * image.size * p)
    coords = [np.random.randint(0, i - 1, int(num_salt))
              for i in image.shape]
    noisy[coords] = 1
    # print(img.shape)
    fig = plt.figure()
    plt.title('Salt image')
    plt.imshow(noisy)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/salt_image.jpg', dpi = 500)

    return noisy
  
def contrast_image(image,contrast=30):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    image[:,:,2] = [[max(pixel - contrast, 0) if pixel < 190 else min(pixel + contrast, 255) for pixel in row] for row in image[:,:,2]]
    image= cv2.cvtColor(image, cv2.COLOR_HSV2BGR)
    # print(img.shape)
    fig = plt.figure()
    plt.title('Contrast image')
    plt.imshow(image)
    plt.show()
    #fig.savefig('/content/drive/MyDrive/Sreekar data/Classification work/Augmented/new/contrast_image.jpg', dpi = 500)

    return image

path=os.path.join('/content/drive/MyDrive/Input Images', '164_06_185_65.jpg' )

img_array = cv2.imread(path)
#print(img_array)
preprocessed = preprocess((img_array))
'''new_array1=vertical_shift(img_array)
new_array2=horizontal_shift(img_array)
new_array3=channel_shift(img_array)
new_array4=brightness(img_array)
new_array5=gausian_blur(img_array)
new_array6=erosion_image(img_array)
new_array7=black_hat_image(img_array)
new_array8=averageing_blur(img_array)
new_array9=sharpen_image(img_array)
new_array10=gaussian_noise(img_array)
new_array11=salt_image(img_array)
new_array12=contrast_image(img_array)
'''

path = '/content/drive/MyDrive/Input Images'
for img in os.listdir('/content/drive/MyDrive/Input Images'):
  img_array=cv2.imread(os.path.join(path,img))
  #print(img_array)
  img_array = whiteblack((img_array))
  print(img_array.shape)
  plt.title(img)
  plt.imshow(img_array)
  break
  plt.show()
  img_array

import pandas as pd
df = pd.read_excel('/content/drive/MyDrive/Project Sem 2 Support Data/All Support Data Final.xlsx')
df.dropna()
print(df.head())

X = df.drop(["ID","Width", "Height", "Images"], axis=1)
y = df.filter(["Width", "Height"], axis = 1)


X

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=45)

from sklearn.ensemble import RandomForestRegressor
rdregressor=RandomForestRegressor()
rdregressor.fit(X_train, y_train)
y_pred = rdregressor.predict(X_test)
print(r2_score(y_test, y_pred))

from pprint import pprint
# Look at parameters used by our current forest
print('Parameters currently in use:\n')
pprint(rdregressor.get_params())

from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
pprint(random_grid)

# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestRegressor()
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations, and use all available cores
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
# Fit the random search model
rf_random.fit(X, y)

rf_random.best_params_

def evaluate(model, test_features, test_labels):
    predictions = model.predict(test_features)
    errors = abs(predictions - test_labels)
    mape = 100 * np.mean(errors / test_labels)
    accuracy = 100 - mape
    print('Model Performance')
    print(np.mean(errors))
    print(accuracy)
    return accuracy

base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)
base_model.fit(X, y)
base_accuracy = evaluate(base_model, X, y)

best_random = rf_random.best_estimator_
random_accuracy = evaluate(best_random, X, y)

y_pred = best_random.predict(X_test)
print(r2_score(y_test, y_pred))

tuned_pred=best_random.predict(X_test)

plt.scatter(y_test,tuned_pred)

from sklearn.tree import DecisionTreeRegressor
dtregressor=DecisionTreeRegressor()
dtregressor.fit(X_train, y_train)
y_pred = dtregressor.predict(X_test)
print(r2_score(y_test, y_pred))

from pprint import pprint
# Look at parameters used by our current forest
print('Parameters currently in use:\n')
pprint(dtregressor.get_params())

parameters={"splitter":["best","random"],
            "max_depth" : [1,3,5,7,9,11,12],
           "min_samples_leaf":[1,2,3,4,5,6,7,8,9,10],
           "min_weight_fraction_leaf":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],
           "max_features":["auto","log2","sqrt",None],
           "max_leaf_nodes":[None,10,20,30,40,50,60,70,80,90] }

import datetime

from sklearn.model_selection import GridSearchCV
tuning_model=GridSearchCV(dtregressor, param_grid=parameters, scoring='neg_mean_squared_error', cv=3, verbose=3)
def timer(start_time=None):
    if not start_time:
        start_time=datetime.now()
        return start_time
    elif start_time:
        thour,temp_sec=divmod((datetime.now()-start_time).total_seconds(),3600)
        tmin,tsec=divmod(temp_sec,60)
        #print(thour,":",tmin,':',round(tsec,2))

from datetime import datetime

start_time=timer(None)

tuning_model.fit(X,y)

timer(start_time)

tuning_model.best_params_

tuning_model.best_score_

tuned_hyper_model= DecisionTreeRegressor(max_depth=7,max_features='auto',max_leaf_nodes=80,min_samples_leaf=1,min_weight_fraction_leaf=0.1,splitter='random')

tuned_hyper_model.fit(X_train,y_train)

tuned_pred=tuned_hyper_model.predict(X_test)

plt.scatter(y_test,tuned_pred)

from sklearn import metrics

print('MAE:', metrics.mean_absolute_error(y_test,tuned_pred))
print('MSE:', metrics.mean_squared_error(y_test, tuned_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, tuned_pred)))

y_pred = tuned_hyper_model.predict(X_test)
print(r2_score(y_test, y_pred))

from sklearn.linear_model import LinearRegression
lrregression = LinearRegression()
lrregression.fit(X_train, y_train)
y_pred = lrregression.predict(X_test)
print(r2_score(y_test, y_pred))

from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.pipeline import Pipeline

# define the model
#Experiment with deeper and wider networks
model = Sequential()
model.add(Dense(128, input_dim=5, activation='relu'))
model.add(Dense(64, activation='selu'))
#Output layer
model.add(Dense(2, activation='linear'))

model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])
model.summary()

history = model.fit(X_train, y_train, validation_split=0.2, epochs =100)

from matplotlib import pyplot as plt
#plot the training and validation accuracy and loss at each epoch
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()


acc = history.history['mae']
val_acc = history.history['val_mae']
plt.plot(epochs, acc, 'y', label='Training MAE')
plt.plot(epochs, val_acc, 'r', label='Validation MAE')
plt.title('Training and validation MAE')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()


############################################
#Predict on test data
predictions = model.predict(X_test[:10])
print("Predicted values are:\n Width     Height \n", predictions)
print("Real values are:\n ", y_test[:10])
##############################################

#Comparison with other models..
#Neural network - from the current code
mse_neural, mae_neural = model.evaluate(X_test, y_test)
print('Mean squared error from neural net: ', mse_neural)
print('Mean absolute error from neural net: ', mae_neural)

y_pred = model.predict(X_test)
print(r2_score(y_test, y_pred))

df1 = pd.read_excel('/content/drive/MyDrive/Project Sem 2 Support Data/All Support Data Image.xlsx')
df1.dropna()
print(df1.head())

X = df1.drop(["ID","Images"], axis=1)
y = df1.filter(["Images"], axis = 1)


y

for path in df1["Images"]:
  for img in os.listdir(path):
   img_array=cv2.imread(os.path.join(path,img))
   #print(img_array)
   img_array = preprocess((img_array))
   img_array = black_hat_image((img_array))
   print(img_array.shape)
   plt.title(img)
   #break
   plt.imshow(img_array)
   plt.show()
   break
  break

dict = {'ID' : [],
        'Scan Speed' : [],
        'Hatch Distance': [],
        'Laser Power' : [],
        'ED' : [],
        'PMD' : [], 
        'Images' : []}

df2 = pd.DataFrame(dict)
display(df2)

img = pd.Series([])
for a in range(len(X)):
  for b in range(len(df1['Images'])):
    if(a == b):
     for img in os.listdir(df1['Images'][b]):
      try:
       img_array=cv2.imread(os.path.join(df1['Images'][b],img))
       #print(img_array)
       img_array = preprocess((img_array))
       img_array = black_hat_image((img_array))
       df3 = {'ID' : df1['ID'][a], 
            'Scan Speed' : X['Scan Speed (mm/s) '][a],
            'Hatch Distance' : X['Hatch Distance (mm)'][a],
            'Laser Power' : X['Laser Power (Watts)'][a],
            'ED' : X['ED'][a],
            'PMD' : X['PMD: Density\ng/CC   with '][a],
            'Images' : img_array}
       df2 = df2.append(df3, ignore_index=True)
      except:
        pass

display(df2)

X = df2.drop(["ID","Images"], axis=1)
y = df2.filter(["Images"], axis = 1)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=45)

